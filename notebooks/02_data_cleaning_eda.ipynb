{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "251f0e94",
   "metadata": {},
   "source": [
    "# üßπ TCS Stock Data - Data Cleaning & Exploratory Data Analysis\n",
    "\n",
    "## Objective\n",
    "This notebook focuses on:\n",
    "- Data cleaning and preprocessing\n",
    "- Missing value handling\n",
    "- Outlier detection and treatment\n",
    "- Comprehensive exploratory data analysis\n",
    "- Interactive visualizations with Plotly\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546b20fe",
   "metadata": {},
   "source": [
    "## üì¶ Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c9ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7460d3d4",
   "metadata": {},
   "source": [
    "## üìÅ Load and Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3f6a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TCS stock data\n",
    "df = pd.read_csv('../data/TCS_stock_history.csv')\n",
    "print(f\"‚úÖ Data loaded: {df.shape}\")\n",
    "\n",
    "# Convert Date to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "# Create a copy for cleaning\n",
    "df_original = df.copy()\n",
    "print(f\"üìä Original data shape: {df_original.shape}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189915be",
   "metadata": {},
   "source": [
    "## üßπ Data Cleaning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf649024",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"üßπ DATA CLEANING PROCESS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Check for missing values\n",
    "print(\"1. Missing Values Analysis:\")\n",
    "missing_before = df.isnull().sum()\n",
    "print(missing_before)\n",
    "\n",
    "# 2. Handle missing values if any\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    print(\"\\nüîß Handling missing values...\")\n",
    "    # Forward fill then backward fill\n",
    "    df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "    print(f\"‚úÖ Missing values after cleaning: {df.isnull().sum().sum()}\")\n",
    "else:\n",
    "    print(\"‚úÖ No missing values found\")\n",
    "\n",
    "# 3. Remove duplicates\n",
    "print(\"\\n2. Duplicate Records:\")\n",
    "duplicates_before = df.duplicated().sum()\n",
    "print(f\"Duplicates before: {duplicates_before}\")\n",
    "\n",
    "if duplicates_before > 0:\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "    print(f\"‚úÖ Duplicates after cleaning: {df.duplicated().sum()}\")\n",
    "else:\n",
    "    print(\"‚úÖ No duplicates found\")\n",
    "\n",
    "# 4. Data type optimization\n",
    "print(\"\\n3. Data Type Optimization:\")\n",
    "print(\"Before:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Optimize numeric columns\n",
    "numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits']\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "print(\"\\nAfter optimization:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(f\"\\nüìä Final cleaned data shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca345ebc",
   "metadata": {},
   "source": [
    "## üîç Outlier Detection & Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fbce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(data, column):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "def detect_outliers_zscore(data, column, threshold=3):\n",
    "    \"\"\"Detect outliers using Z-score method\"\"\"\n",
    "    z_scores = np.abs(stats.zscore(data[column]))\n",
    "    outliers = data[z_scores > threshold]\n",
    "    return outliers\n",
    "\n",
    "# Analyze outliers for each numeric column\n",
    "print(\"=\" * 50)\n",
    "print(\"üîç OUTLIER DETECTION & ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "outlier_summary = {}\n",
    "price_volume_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "\n",
    "for col in price_volume_cols:\n",
    "    if col in df.columns:\n",
    "        # IQR method\n",
    "        outliers_iqr, lower, upper = detect_outliers_iqr(df, col)\n",
    "        \n",
    "        # Z-score method\n",
    "        outliers_zscore = detect_outliers_zscore(df, col)\n",
    "        \n",
    "        outlier_summary[col] = {\n",
    "            'IQR_outliers': len(outliers_iqr),\n",
    "            'IQR_percentage': len(outliers_iqr) / len(df) * 100,\n",
    "            'ZScore_outliers': len(outliers_zscore),\n",
    "            'ZScore_percentage': len(outliers_zscore) / len(df) * 100,\n",
    "            'lower_bound': lower,\n",
    "            'upper_bound': upper\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  IQR outliers: {len(outliers_iqr)} ({len(outliers_iqr)/len(df)*100:.1f}%)\")\n",
    "        print(f\"  Z-Score outliers: {len(outliers_zscore)} ({len(outliers_zscore)/len(df)*100:.1f}%)\")\n",
    "        print(f\"  IQR bounds: [{lower:.2f}, {upper:.2f}]\")\n",
    "\n",
    "# Create outlier summary DataFrame\n",
    "outlier_df = pd.DataFrame(outlier_summary).T\n",
    "print(\"\\nüìä Outlier Summary:\")\n",
    "display(outlier_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014265ca",
   "metadata": {},
   "source": [
    "## üìä Comprehensive EDA - Price Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93a5454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive price analysis visualization\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('TCS Stock Price Trend', 'Price Volatility Analysis',\n",
    "                   'OHLC Candlestick Chart', 'Price Range Analysis'),\n",
    "    specs=[[{'secondary_y': True}, {'secondary_y': False}],\n",
    "           [{'secondary_y': False}, {'secondary_y': False}]]\n",
    ")\n",
    "\n",
    "# 1. Price trend with volume\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df['Date'], y=df['Close'], name='Close Price',\n",
    "              line=dict(color='#2E86AB', width=2)),\n",
    "    row=1, col=1, secondary_y=False\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df['Date'], y=df['Volume'], name='Volume',\n",
    "              line=dict(color='#A23B72', width=1, dash='dot')),\n",
    "    row=1, col=1, secondary_y=True\n",
    ")\n",
    "\n",
    "# 2. Daily price change\n",
    "df['Daily_Change'] = df['Close'].pct_change() * 100\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df['Date'], y=df['Daily_Change'], name='Daily % Change',\n",
    "              line=dict(color='#F18F01', width=1)),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Candlestick chart (sample last 100 days)\n",
    "sample_df = df.tail(100)\n",
    "fig.add_trace(\n",
    "    go.Candlestick(x=sample_df['Date'],\n",
    "                   open=sample_df['Open'],\n",
    "                   high=sample_df['High'],\n",
    "                   low=sample_df['Low'],\n",
    "                   close=sample_df['Close'],\n",
    "                   name='OHLC'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Price range (High-Low)\n",
    "df['Price_Range'] = df['High'] - df['Low']\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df['Date'], y=df['Price_Range'], name='Daily Range',\n",
    "              line=dict(color='#C73E1D', width=1)),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=\"TCS Stock - Comprehensive Price Analysis\",\n",
    "    title_x=0.5,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "# Update y-axes labels\n",
    "fig.update_yaxes(title_text=\"Price (‚Çπ)\", row=1, col=1, secondary_y=False)\n",
    "fig.update_yaxes(title_text=\"Volume\", row=1, col=1, secondary_y=True)\n",
    "fig.update_yaxes(title_text=\"% Change\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Price (‚Çπ)\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Range (‚Çπ)\", row=2, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cc2186",
   "metadata": {},
   "source": [
    "## üìà Volume Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb48d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volume analysis\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Volume Over Time', 'Volume Distribution',\n",
    "                   'Price vs Volume Relationship', 'Volume Anomalies'),\n",
    "    specs=[[{'secondary_y': False}, {'secondary_y': False}],\n",
    "           [{'secondary_y': False}, {'secondary_y': False}]]\n",
    ")\n",
    "\n",
    "# 1. Volume over time\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df['Date'], y=df['Volume'], name='Volume',\n",
    "              line=dict(color='#FF6B35', width=1)),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Volume distribution\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=df['Volume'], name='Volume Distribution',\n",
    "                marker_color='#004E89', nbinsx=50),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Price vs Volume scatter\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df['Volume'], y=df['Close'], mode='markers',\n",
    "              name='Price vs Volume',\n",
    "              marker=dict(color='#009639', size=4, opacity=0.6)),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Volume anomalies (extremely high/low volume days)\n",
    "volume_mean = df['Volume'].mean()\n",
    "volume_std = df['Volume'].std()\n",
    "high_volume = df[df['Volume'] > (volume_mean + 2 * volume_std)]\n",
    "low_volume = df[df['Volume'] < (volume_mean - 2 * volume_std)]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=high_volume['Date'], y=high_volume['Volume'],\n",
    "              mode='markers', name='High Volume Days',\n",
    "              marker=dict(color='red', size=8)),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=low_volume['Date'], y=low_volume['Volume'],\n",
    "              mode='markers', name='Low Volume Days',\n",
    "              marker=dict(color='blue', size=8)),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Add average volume line\n",
    "fig.add_hline(y=volume_mean, line_dash=\"dash\", line_color=\"gray\",\n",
    "              annotation_text=\"Average Volume\", row=2, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=\"TCS Stock - Volume Analysis\",\n",
    "    title_x=0.5,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Volume statistics\n",
    "print(\"üìä Volume Statistics:\")\n",
    "print(f\"Average Volume: {df['Volume'].mean():,.0f}\")\n",
    "print(f\"Median Volume: {df['Volume'].median():,.0f}\")\n",
    "print(f\"Max Volume: {df['Volume'].max():,.0f}\")\n",
    "print(f\"Min Volume: {df['Volume'].min():,.0f}\")\n",
    "print(f\"High Volume Days (>2œÉ): {len(high_volume)}\")\n",
    "print(f\"Low Volume Days (<-2œÉ): {len(low_volume)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35483134",
   "metadata": {},
   "source": [
    "## üî• Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7211d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "correlation_matrix = df[numeric_cols].corr()\n",
    "\n",
    "# Create interactive correlation heatmap\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=correlation_matrix.values,\n",
    "    x=correlation_matrix.columns,\n",
    "    y=correlation_matrix.columns,\n",
    "    colorscale='RdBu',\n",
    "    zmid=0,\n",
    "    text=correlation_matrix.round(3).values,\n",
    "    texttemplate=\"%{text}\",\n",
    "    textfont={\"size\": 12},\n",
    "    hoverongaps=False\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"TCS Stock - Correlation Heatmap\",\n",
    "    title_x=0.5,\n",
    "    width=600,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Display correlation insights\n",
    "print(\"üîç Key Correlation Insights:\")\n",
    "print(f\"Open-Close correlation: {correlation_matrix.loc['Open', 'Close']:.3f}\")\n",
    "print(f\"High-Low correlation: {correlation_matrix.loc['High', 'Low']:.3f}\")\n",
    "print(f\"Volume-Close correlation: {correlation_matrix.loc['Volume', 'Close']:.3f}\")\n",
    "print(f\"High-Close correlation: {correlation_matrix.loc['High', 'Close']:.3f}\")\n",
    "print(f\"Low-Close correlation: {correlation_matrix.loc['Low', 'Close']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adf797b",
   "metadata": {},
   "source": [
    "## üìÖ Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e6f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add temporal features\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
    "df['Quarter'] = df['Date'].dt.quarter\n",
    "\n",
    "# Year-wise analysis\n",
    "yearly_stats = df.groupby('Year').agg({\n",
    "    'Close': ['mean', 'min', 'max', 'std'],\n",
    "    'Volume': ['mean', 'sum'],\n",
    "    'Date': 'count'\n",
    "}).round(2)\n",
    "\n",
    "yearly_stats.columns = ['Avg_Close', 'Min_Close', 'Max_Close', 'Close_Std', 'Avg_Volume', 'Total_Volume', 'Trading_Days']\n",
    "\n",
    "print(\"üìä Year-wise Performance:\")\n",
    "display(yearly_stats)\n",
    "\n",
    "# Monthly seasonality\n",
    "monthly_returns = df.groupby('Month')['Close'].mean()\n",
    "\n",
    "# Day of week analysis\n",
    "day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "dow_analysis = df.groupby('DayOfWeek').agg({\n",
    "    'Close': 'mean',\n",
    "    'Volume': 'mean',\n",
    "    'Daily_Change': 'mean'\n",
    "}).round(2)\n",
    "dow_analysis.index = [day_names[i] for i in dow_analysis.index]\n",
    "\n",
    "# Create temporal analysis visualization\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Year-wise Average Close Price', 'Monthly Seasonality',\n",
    "                   'Day of Week Analysis', 'Quarterly Performance'),\n",
    "    specs=[[{'secondary_y': False}, {'secondary_y': False}],\n",
    "           [{'secondary_y': True}, {'secondary_y': False}]]\n",
    ")\n",
    "\n",
    "# Year-wise performance\n",
    "fig.add_trace(\n",
    "    go.Bar(x=yearly_stats.index, y=yearly_stats['Avg_Close'],\n",
    "           name='Avg Close Price', marker_color='#1f77b4'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Monthly seasonality\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=monthly_returns.index, y=monthly_returns.values,\n",
    "              mode='lines+markers', name='Monthly Avg Close',\n",
    "              line=dict(color='#ff7f0e', width=3)),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Day of week analysis\n",
    "fig.add_trace(\n",
    "    go.Bar(x=dow_analysis.index, y=dow_analysis['Close'],\n",
    "           name='Avg Close', marker_color='#2ca02c'),\n",
    "    row=2, col=1, secondary_y=False\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=dow_analysis.index, y=dow_analysis['Volume'],\n",
    "              mode='lines+markers', name='Avg Volume',\n",
    "              line=dict(color='#d62728', width=2)),\n",
    "    row=2, col=1, secondary_y=True\n",
    ")\n",
    "\n",
    "# Quarterly performance\n",
    "quarterly_stats = df.groupby('Quarter')['Close'].mean()\n",
    "fig.add_trace(\n",
    "    go.Bar(x=['Q1', 'Q2', 'Q3', 'Q4'], y=quarterly_stats.values,\n",
    "           name='Quarterly Avg', marker_color='#9467bd'),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=\"TCS Stock - Temporal Analysis\",\n",
    "    title_x=0.5,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nüìÖ Day of Week Analysis:\")\n",
    "display(dow_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9244e7da",
   "metadata": {},
   "source": [
    "## üí∞ Dividends & Stock Splits Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e703de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dividends and stock splits\n",
    "dividend_data = df[df['Dividends'] > 0].copy() if 'Dividends' in df.columns else pd.DataFrame()\n",
    "splits_data = df[df['Stock Splits'] > 0].copy() if 'Stock Splits' in df.columns else pd.DataFrame()\n",
    "\n",
    "if not dividend_data.empty or not splits_data.empty:\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        subplot_titles=('Dividends Over Time', 'Stock Splits Timeline'),\n",
    "        specs=[[{'secondary_y': True}], [{'secondary_y': False}]]\n",
    "    )\n",
    "    \n",
    "    # Plot stock price as background\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df['Date'], y=df['Close'], name='Close Price',\n",
    "                  line=dict(color='lightgray', width=1), opacity=0.5),\n",
    "        row=1, col=1, secondary_y=False\n",
    "    )\n",
    "    \n",
    "    if not dividend_data.empty:\n",
    "        # Dividend events\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=dividend_data['Date'], y=dividend_data['Dividends'],\n",
    "                      mode='markers+lines', name='Dividends',\n",
    "                      marker=dict(color='green', size=10, symbol='diamond'),\n",
    "                      line=dict(color='green', width=2)),\n",
    "            row=1, col=1, secondary_y=True\n",
    "        )\n",
    "        \n",
    "        print(f\"üìä Dividend Summary:\")\n",
    "        print(f\"Total dividend events: {len(dividend_data)}\")\n",
    "        print(f\"Total dividends paid: ‚Çπ{dividend_data['Dividends'].sum():.2f}\")\n",
    "        print(f\"Average dividend: ‚Çπ{dividend_data['Dividends'].mean():.2f}\")\n",
    "        print(f\"Highest dividend: ‚Çπ{dividend_data['Dividends'].max():.2f}\")\n",
    "    \n",
    "    if not splits_data.empty:\n",
    "        # Stock split events\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=splits_data['Date'], y=splits_data['Stock Splits'],\n",
    "                      mode='markers', name='Stock Splits',\n",
    "                      marker=dict(color='purple', size=15, symbol='star')),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüìä Stock Split Summary:\")\n",
    "        print(f\"Total split events: {len(splits_data)}\")\n",
    "        print(f\"Split ratios: {splits_data['Stock Splits'].unique()}\")\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=600,\n",
    "        title_text=\"TCS Stock - Corporate Actions Analysis\",\n",
    "        title_x=0.5\n",
    "    )\n",
    "    \n",
    "    fig.update_yaxes(title_text=\"Price (‚Çπ)\", row=1, col=1, secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"Dividend (‚Çπ)\", row=1, col=1, secondary_y=True)\n",
    "    fig.update_yaxes(title_text=\"Split Ratio\", row=2, col=1)\n",
    "    \n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No dividend or stock split data available in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fda34a1",
   "metadata": {},
   "source": [
    "## üìà Statistical Analysis Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecec897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive statistical summary\n",
    "print(\"=\" * 60)\n",
    "print(\"üìà COMPREHENSIVE STATISTICAL ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Price statistics\n",
    "print(\"üí∞ PRICE ANALYSIS:\")\n",
    "print(f\"Price Range: ‚Çπ{df['Close'].min():.2f} - ‚Çπ{df['Close'].max():.2f}\")\n",
    "print(f\"Total Return: {((df['Close'].iloc[-1] / df['Close'].iloc[0]) - 1) * 100:.1f}%\")\n",
    "print(f\"Annualized Return: {(((df['Close'].iloc[-1] / df['Close'].iloc[0]) ** (365.25 / (df['Date'].iloc[-1] - df['Date'].iloc[0]).days)) - 1) * 100:.1f}%\")\n",
    "print(f\"Average Daily Return: {df['Daily_Change'].mean():.3f}%\")\n",
    "print(f\"Volatility (Daily): {df['Daily_Change'].std():.3f}%\")\n",
    "print(f\"Annualized Volatility: {df['Daily_Change'].std() * np.sqrt(252):.1f}%\")\n",
    "\n",
    "# Volume statistics\n",
    "print(\"\\nüìä VOLUME ANALYSIS:\")\n",
    "print(f\"Average Daily Volume: {df['Volume'].mean():,.0f} shares\")\n",
    "print(f\"Total Volume Traded: {df['Volume'].sum():,.0f} shares\")\n",
    "print(f\"Volume Volatility: {df['Volume'].std():,.0f}\")\n",
    "\n",
    "# Risk metrics\n",
    "print(\"\\n‚ö†Ô∏è RISK METRICS:\")\n",
    "negative_returns = df['Daily_Change'][df['Daily_Change'] < 0]\n",
    "positive_returns = df['Daily_Change'][df['Daily_Change'] > 0]\n",
    "\n",
    "print(f\"Downside Days: {len(negative_returns)} ({len(negative_returns)/len(df)*100:.1f}%)\")\n",
    "print(f\"Upside Days: {len(positive_returns)} ({len(positive_returns)/len(df)*100:.1f}%)\")\n",
    "print(f\"Maximum Daily Gain: {df['Daily_Change'].max():.2f}%\")\n",
    "print(f\"Maximum Daily Loss: {df['Daily_Change'].min():.2f}%\")\n",
    "\n",
    "# Sharpe ratio (assuming risk-free rate of 5%)\n",
    "risk_free_rate = 5  # 5% annual\n",
    "daily_rf = risk_free_rate / 252 / 100\n",
    "excess_returns = df['Daily_Change'] / 100 - daily_rf\n",
    "sharpe_ratio = excess_returns.mean() / excess_returns.std() * np.sqrt(252)\n",
    "print(f\"Sharpe Ratio (5% RF): {sharpe_ratio:.3f}\")\n",
    "\n",
    "# Summary insights\n",
    "print(\"\\nüéØ KEY INSIGHTS:\")\n",
    "print(f\"‚Ä¢ Data Quality: {100 - (df.isnull().sum().sum() / df.size * 100):.1f}% complete\")\n",
    "print(f\"‚Ä¢ Trading Period: {(df['Date'].max() - df['Date'].min()).days} days\")\n",
    "print(f\"‚Ä¢ Price Trend: {'Bullish' if df['Close'].iloc[-1] > df['Close'].iloc[0] else 'Bearish'}\")\n",
    "print(f\"‚Ä¢ Volatility Level: {'High' if df['Daily_Change'].std() > 3 else 'Moderate' if df['Daily_Change'].std() > 1.5 else 'Low'}\")\n",
    "\n",
    "print(\"\\n‚úÖ DATA CLEANING & EDA COMPLETED SUCCESSFULLY!\")\n",
    "print(\"üîÑ Next: Feature Engineering (03_feature_engineering.ipynb)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9391fe86",
   "metadata": {},
   "source": [
    "## üíæ Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1692f17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data for next notebooks\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Remove temporary columns if any\n",
    "temp_cols = ['Year', 'Month', 'DayOfWeek', 'Quarter', 'Daily_Change', 'Price_Range']\n",
    "df_clean = df_clean.drop(columns=[col for col in temp_cols if col in df_clean.columns])\n",
    "\n",
    "# Save to CSV\n",
    "df_clean.to_csv('../data/TCS_stock_cleaned.csv', index=False)\n",
    "print(f\"‚úÖ Cleaned data saved: {df_clean.shape}\")\n",
    "print(\"üìÅ File: ../data/TCS_stock_cleaned.csv\")\n",
    "\n",
    "# Display final structure\n",
    "print(\"\\nüìã Final Cleaned Dataset:\")\n",
    "print(f\"Shape: {df_clean.shape}\")\n",
    "print(f\"Columns: {list(df_clean.columns)}\")\n",
    "print(f\"Date Range: {df_clean['Date'].min()} to {df_clean['Date'].max()}\")\n",
    "print(f\"Missing Values: {df_clean.isnull().sum().sum()}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
